{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio UI for the RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradio UI for the RAG Chatbot\n",
    "Provides a web interface for the chatbot, deployable on Hugging Face Spaces\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gradio as gr\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from llama_model import LlamaInstructModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import pinecone\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"ragbot.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"ragbot_gradio\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Constants and configurations\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "PINECONE_INDEX = \"gpt-conversations\"\n",
    "MODEL_NAME = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "    \n",
    "    def generate_embeddings(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a text\"\"\"\n",
    "        return self.model.encode(text)\n",
    "\n",
    "class PineconeClient:\n",
    "    def __init__(self, api_key: str, environment: str, index_name: str):\n",
    "        self.api_key = api_key\n",
    "        self.environment = environment\n",
    "        self.index_name = index_name\n",
    "        self._initialize()\n",
    "        \n",
    "    def _initialize(self):\n",
    "        try:\n",
    "            logger.info(f\"Initializing Pinecone: {self.environment}\")\n",
    "            pinecone.init(api_key=self.api_key, environment=self.environment)\n",
    "            self.index = pinecone.Index(self.index_name)\n",
    "            logger.info(f\"Connected to Pinecone index: {self.index_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to Pinecone: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def query(self, vector: List[float], top_k: int = 3) -> Dict:\n",
    "        \"\"\"Query the index with a vector\"\"\"\n",
    "        try:\n",
    "            return self.index.query(vector=vector, top_k=top_k, include_metadata=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error querying Pinecone: {str(e)}\")\n",
    "            return {\"matches\": []}\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing RAG Chatbot\")\n",
    "        self.processor = DataProcessor()\n",
    "        self.pinecone_client = PineconeClient(\n",
    "            api_key=PINECONE_API_KEY,\n",
    "            environment=PINECONE_ENVIRONMENT,\n",
    "            index_name=PINECONE_INDEX\n",
    "        )\n",
    "        self.llm = None\n",
    "    \n",
    "    def _get_llm(self):\n",
    "        \"\"\"Lazy initialization of the language model\"\"\"\n",
    "        if self.llm is None:\n",
    "            logger.info(f\"Loading LLM: {MODEL_NAME}\")\n",
    "            self.llm = LlamaInstructModel(model_name=MODEL_NAME)\n",
    "        return self.llm\n",
    "    \n",
    "    def process_query(self, query: str, history: List = None):\n",
    "        \"\"\"Process a query using RAG and return a response\"\"\"\n",
    "        if history is None:\n",
    "            history = []\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Processing query: {query[:50]}...\")\n",
    "            \n",
    "            # Generate embedding for the query\n",
    "            processed_query = self.processor.preprocess_text(query)\n",
    "            query_embedding = self.processor.generate_embeddings(processed_query).tolist()\n",
    "            \n",
    "            # Retrieve relevant documents\n",
    "            results = self.pinecone_client.query(vector=query_embedding, top_k=3)\n",
    "            \n",
    "            # Extract contexts\n",
    "            contexts = []\n",
    "            for match in results.get(\"matches\", []):\n",
    "                if \"metadata\" in match and \"text\" in match[\"metadata\"]:\n",
    "                    contexts.append(match[\"metadata\"][\"text\"])\n",
    "            \n",
    "            # Format system and user prompts\n",
    "            system_prompt = \"You are a helpful assistant. Answer the question accurately based on the provided context.\"\n",
    "            \n",
    "            user_prompt = f\"\"\"\n",
    "            Answer based on the following contexts:\n",
    "            \n",
    "            {' '.join(contexts)}\n",
    "            \n",
    "            Question: {query}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Generate response\n",
    "            llm = self._get_llm()\n",
    "            response = llm.generate_response(system_prompt, user_prompt)\n",
    "            \n",
    "            # Format sources as markdown for display\n",
    "            sources_md = \"\\n\\n**Sources:**\\n\"\n",
    "            for i, match in enumerate(results.get(\"matches\", [])[:3]):\n",
    "                if \"metadata\" in match:\n",
    "                    score = float(match.get(\"score\", 0)) * 100\n",
    "                    text = match[\"metadata\"].get(\"text\", \"\")\n",
    "                    # Truncate long texts\n",
    "                    if len(text) > 200:\n",
    "                        text = text[:200] + \"...\"\n",
    "                    sources_md += f\"- **Source {i+1}** (Relevance: {score:.1f}%): {text}\\n\"\n",
    "            \n",
    "            # Append sources to response\n",
    "            if results.get(\"matches\"):\n",
    "                response += sources_md\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\", exc_info=True)\n",
    "            return \"I'm sorry, I encountered an error while processing your query. Please try again.\"\n",
    "\n",
    "# Initialize the chatbot\n",
    "chatbot = RAGChatbot()\n",
    "\n",
    "# Define the Gradio chat interface\n",
    "def respond(message, history):\n",
    "    \"\"\"Handle incoming messages and generate responses\"\"\"\n",
    "    return chatbot.process_query(message, history)\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    respond,\n",
    "    title=\"RAG Chatbot\",\n",
    "    description=\"A Retrieval-Augmented Generation chatbot using Llama Instruct and Pinecone\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\n",
    "        \"What information do you have about machine learning?\",\n",
    "        \"Tell me about natural language processing\",\n",
    "        \"Explain the concept of retrieval-augmented generation\"\n",
    "    ],\n",
    "    retry_btn=None,\n",
    "    undo_btn=None,\n",
    "    clear_btn=\"Clear\",\n",
    ")\n",
    "\n",
    "# For Hugging Face Spaces deployment\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entry Point for the RAG Chatbot Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Entry point for the RAG Chatbot application\n",
    "For Hugging Face Spaces deployment\n",
    "\"\"\"\n",
    "\n",
    "from gradio_app import demo\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Instruct Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Llama-Instruct Model Wrapper\n",
    "Provides an interface to a locally loaded Llama-based instruction-tuned model\n",
    "from Hugging Face as a replacement for OpenAI's API.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "logger = logging.getLogger(\"ragbot\")\n",
    "\n",
    "class LlamaInstructModel:\n",
    "    \"\"\"Wrapper for Llama-based instruction-tuned models from Hugging Face\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"NousResearch/Llama-2-7b-chat-hf\", \n",
    "                 device=\"auto\", \n",
    "                 use_4bit=True,\n",
    "                 max_new_tokens=500):\n",
    "        \"\"\"\n",
    "        Initialize the Llama model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name/path\n",
    "            device: 'cpu', 'cuda', 'auto' for automatic detection\n",
    "            use_4bit: Whether to use 4-bit quantization (reduced memory usage)\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing Llama model: {model_name}\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        \n",
    "        # Determine device placement\n",
    "        if device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Configure quantization for memory efficiency\n",
    "        quantization_config = None\n",
    "        if use_4bit and self.device == \"cuda\":\n",
    "            logger.info(\"Using 4-bit quantization\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "        \n",
    "        # Load the tokenizer and model\n",
    "        try:\n",
    "            logger.info(\"Loading tokenizer\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name, \n",
    "                padding_side=\"left\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if not self.tokenizer.pad_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                \n",
    "            logger.info(\"Loading model\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=self.device if self.device == \"cuda\" else None,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            if self.device == \"cpu\":\n",
    "                logger.info(\"Moving model to CPU\")\n",
    "                self.model = self.model.to(\"cpu\")\n",
    "                \n",
    "            logger.info(\"Model and tokenizer loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "            \n",
    "    def generate_response(self, system_prompt: str, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the Llama model.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System instructions/context\n",
    "            user_message: User query\n",
    "            \n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Generating response with Llama model\")\n",
    "            \n",
    "            # Format the prompt according to the model's expected format\n",
    "            if \"llama-2\" in self.model_name.lower():\n",
    "                # Llama-2 chat format\n",
    "                prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_message} [/INST]\"\n",
    "            elif \"mistral\" in self.model_name.lower():\n",
    "                # Mistral Instruct format\n",
    "                prompt = f\"<s>[INST] {system_prompt}\\n\\n{user_message} [/INST]\"\n",
    "            elif \"llama-3\" in self.model_name.lower():\n",
    "                # Llama-3 format\n",
    "                prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{user_message}\\n<|assistant|>\"\n",
    "            else:\n",
    "                # Generic instruction format\n",
    "                prompt = f\"System: {system_prompt}\\n\\nUser: {user_message}\\n\\nAssistant:\"\n",
    "            \n",
    "            logger.debug(f\"Formatted prompt: {prompt[:100]}...\")\n",
    "            \n",
    "            # Tokenize the prompt\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            # Move inputs to the correct device\n",
    "            if self.device != \"cpu\":\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract just the assistant's reply\n",
    "            if \"llama-2\" in self.model_name.lower():\n",
    "                response = response.split(\"[/INST]\")[-1].strip()\n",
    "            elif \"mistral\" in self.model_name.lower():\n",
    "                response = response.split(\"[/INST]\")[-1].strip()\n",
    "            elif \"llama-3\" in self.model_name.lower():\n",
    "                response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "            else:\n",
    "                response = response.split(\"Assistant:\")[-1].strip()\n",
    "            \n",
    "            logger.info(f\"Generated response: {response[:50]}...\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {str(e)}\", exc_info=True)\n",
    "            return \"I apologize, but I encountered an error while generating a response. Please try again.\"\n",
    "            \n",
    "    def chat_completion_create(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a chat completion response similar to OpenAI API format.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of messages in the format [{\"role\": \"system\", \"content\": \"...\"}, ...]\n",
    "            \n",
    "        Returns:\n",
    "            Dict with response in a format similar to OpenAI's API\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract system prompt and user message\n",
    "            system_content = \"\"\n",
    "            user_content = \"\"\n",
    "            \n",
    "            # Process messages to extract system and user content\n",
    "            for msg in messages:\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    system_content = msg[\"content\"]\n",
    "                elif msg[\"role\"] == \"user\":\n",
    "                    # For simplicity, we'll use the last user message\n",
    "                    user_content = msg[\"content\"]\n",
    "            \n",
    "            # If no system prompt was provided, use a default one\n",
    "            if not system_content:\n",
    "                system_content = \"You are a helpful, harmless, and precise assistant.\"\n",
    "                \n",
    "            # Generate the response\n",
    "            assistant_reply = self.generate_response(system_content, user_content)\n",
    "            \n",
    "            # Format response to match OpenAI's structure\n",
    "            response = {\n",
    "                \"choices\": [\n",
    "                    {\n",
    "                        \"message\": {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": assistant_reply\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in chat completion: {str(e)}\", exc_info=True)\n",
    "            # Return a minimal response structure with error message\n",
    "            return {\n",
    "                \"choices\": [\n",
    "                    {\n",
    "                        \"message\": {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": \"I apologize, but I encountered an error while processing your request.\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy==1.24.3\n",
    "scikit-learn==1.2.2\n",
    "sentence-transformers==2.2.2\n",
    "pinecone-client==2.2.4\n",
    "python-dotenv==1.0.0\n",
    "nltk==3.8.1\n",
    "python-dateutil==2.8.2\n",
    "fastapi==0.103.1\n",
    "uvicorn==0.23.2\n",
    "openai==1.3.0\n",
    "jinja2==3.1.2\n",
    "pydantic==2.4.2\n",
    "starlette==0.27.0\n",
    "email-validator==2.0.0\n",
    "gradio==4.13.0\n",
    "transformers==4.35.2\n",
    "torch==2.1.0\n",
    "accelerate==0.23.0\n",
    "bitsandbytes==0.41.1\n",
    "huggingface-hub==0.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot System with Pinecone and Mobile Interface - Part 1: Data Processing and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chatbot System with Pinecone and Mobile Interface\n",
    "\n",
    "# Part 1: Data Processing and Embedding\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "from dateutil.parser import parse as date_parse\n",
    "import email.utils\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"ragbot.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"ragbot\")\n",
    "\n",
    "# Log startup\n",
    "logger.info(\"RAG Chatbot System initializing...\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "logger.info(\"Environment variables loaded from .env file\")\n",
    "\n",
    "# Validate required environment variables\n",
    "def validate_environment():\n",
    "    \"\"\"Validate that all required environment variables are set.\"\"\"\n",
    "    required_vars = [\n",
    "        \"PINECONE_API_KEY\", \n",
    "        \"PINECONE_ENVIRONMENT\", \n",
    "        \"OPENAI_API_KEY\"\n",
    "    ]\n",
    "    \n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    \n",
    "    if missing_vars:\n",
    "        logger.error(\"Missing required environment variables: %s\", \", \".join(missing_vars))\n",
    "        print(\"ERROR: The following required environment variables are missing:\")\n",
    "        for var in missing_vars:\n",
    "            print(f\"  - {var}\")\n",
    "        print(\"\\nPlease create a .env file with these variables or set them in your environment.\")\n",
    "        print(\"Example .env file:\")\n",
    "        print(\"PINECONE_API_KEY=your_pinecone_api_key\")\n",
    "        print(\"PINECONE_ENVIRONMENT=your_pinecone_environment\")\n",
    "        print(\"OPENAI_API_KEY=your_openai_api_key\")\n",
    "        return False\n",
    "    \n",
    "    logger.info(\"All required environment variables are set\")\n",
    "    return True\n",
    "\n",
    "# Paperspace data configuration\n",
    "PAPERSPACE_STORAGE = {\n",
    "    \"project_id\": \"t9hrtpxj9\",\n",
    "    \"storage_id\": \"dsm97c6ujy7vf8\",\n",
    "    \"storage_type\": \"s3\",\n",
    "    \"storage_bucket\": \"t9hrtpxj9\",\n",
    "    \"storage_access_key\": \"0DOF5K7A04RUSD5R1SC0\"\n",
    "}\n",
    "\n",
    "# Configure data paths\n",
    "# For Paperspace, data is typically mounted at /storage\n",
    "PAPERSPACE_DATA_PATH = \"/storage/datasets/combined_documents_deduplicated.json\"\n",
    "# Fallback to local data if not running on Paperspace\n",
    "LOCAL_DATA_PATH = \"combined_documents_deduplicated.json\"\n",
    "\n",
    "# Determine which data path to use\n",
    "def get_data_path():\n",
    "    \"\"\"Determine the correct data path based on the environment.\n",
    "    Returns the appropriate path for data files depending on whether \n",
    "    running on Paperspace or locally.\n",
    "    \"\"\"\n",
    "    # Check for Paperspace environment variables as primary indicator\n",
    "    if os.environ.get('PAPERSPACE_NOTEBOOK_REPO') or os.environ.get('PAPERSPACE_GRADIENT_RUN_ID'):\n",
    "        logger.info(\"Detected Paperspace environment variables\")\n",
    "        return PAPERSPACE_DATA_PATH\n",
    "    \n",
    "    # Secondary check: verify /storage mount point exists and is accessible\n",
    "    if os.path.exists(\"/storage\") and os.access(\"/storage\", os.R_OK):\n",
    "        try:\n",
    "            # Try to list the directory to confirm it's properly mounted\n",
    "            storage_contents = os.listdir(\"/storage\")\n",
    "            if storage_contents:\n",
    "                logger.info(\"Using Paperspace mounted storage\")\n",
    "                \n",
    "                # Check if the specific dataset file exists\n",
    "                if os.path.exists(PAPERSPACE_DATA_PATH):\n",
    "                    logger.info(\"Found dataset at %s\", PAPERSPACE_DATA_PATH)\n",
    "                    return PAPERSPACE_DATA_PATH\n",
    "                else:\n",
    "                    logger.warning(\"Dataset file not found at %s\", PAPERSPACE_DATA_PATH)\n",
    "                    logger.info(\"Searching for alternative dataset files...\")\n",
    "                    \n",
    "                    # Search for any JSON files in datasets directory\n",
    "                    datasets_dir = \"/storage/datasets\"\n",
    "                    if os.path.exists(datasets_dir):\n",
    "                        json_files = [f for f in os.listdir(datasets_dir) if f.endswith('.json')]\n",
    "                        if json_files:\n",
    "                            alt_dataset = os.path.join(datasets_dir, json_files[0])\n",
    "                            logger.info(\"Found alternative dataset file: %s\", alt_dataset)\n",
    "                            return alt_dataset\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error accessing Paperspace storage: %s\", str(e), exc_info=True)\n",
    "\n",
    "    logger.info(\"Using local storage at %s\", LOCAL_DATA_PATH)\n",
    "    return LOCAL_DATA_PATH\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def load_conversations(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load conversations from a JSON file\"\"\"\n",
    "        print(f\"Loading data from: {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def extract_keywords(self, texts: List[str], top_n: int = 10) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract keywords using TF-IDF\"\"\"\n",
    "        vectorizer = TfidfVectorizer(max_features=100)\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        keywords_dict = {}\n",
    "        \n",
    "        for i, doc in enumerate(texts):\n",
    "            tfidf_scores = zip(feature_names, tfidf_matrix[i].toarray()[0])\n",
    "            sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "            keywords_dict[i] = [word for word, score in sorted_scores[:top_n]]\n",
    "            \n",
    "        return keywords_dict\n",
    "    \n",
    "    def extract_dates(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract dates from text using multiple patterns and formats.\n",
    "        Returns a list of ISO format dates (YYYY-MM-DD).\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Common date patterns\n",
    "        date_patterns = [\n",
    "            # ISO format: YYYY-MM-DD\n",
    "            r'\\d{4}-\\d{1,2}-\\d{1,2}',\n",
    "            # MM/DD/YYYY or DD/MM/YYYY\n",
    "            r'\\d{1,2}/\\d{1,2}/\\d{4}',\n",
    "            # Month name formats: Jan 1, 2023 or January 1st, 2023\n",
    "            r'(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s+\\d{4}',\n",
    "            # DD-MM-YYYY\n",
    "            r'\\d{1,2}-\\d{1,2}-\\d{4}',\n",
    "            # Email date format: Tue, 01 Mar 2022 12:34:56\n",
    "            r'(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun),\\s+\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}',\n",
    "        ]\n",
    "        \n",
    "        extracted_dates = []\n",
    "        \n",
    "        # Extract dates using patterns\n",
    "        for pattern in date_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                date_str = match.group(0)\n",
    "                try:\n",
    "                    # Try to parse the date string\n",
    "                    if ',' in date_str and any(day in date_str for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']):\n",
    "                        # Handle email date format\n",
    "                        parsed_date = email.utils.parsedate_to_datetime(date_str)\n",
    "                        date_iso = parsed_date.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        # Handle other formats\n",
    "                        parsed_date = date_parse(date_str, fuzzy=True)\n",
    "                        date_iso = parsed_date.strftime('%Y-%m-%d')\n",
    "                    \n",
    "                    extracted_dates.append(date_iso)\n",
    "                except (ValueError, OverflowError):\n",
    "                    # Skip dates that can't be parsed\n",
    "                    continue\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_dates = [date for date in extracted_dates if not (date in seen or seen.add(date))]\n",
    "        \n",
    "        return unique_dates\n",
    "    \n",
    "    def extract_people(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract potential people names from text.\n",
    "        Uses a combination of regex patterns and NLTK Named Entity Recognition.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        people = set()\n",
    "        \n",
    "        # Method 1: Use NLTK's named entity recognition\n",
    "        try:\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                named_entities = nltk.ne_chunk(tagged)\n",
    "                \n",
    "                # Extract person names\n",
    "                for chunk in named_entities:\n",
    "                    if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                        name = ' '.join([c[0] for c in chunk])\n",
    "                        if len(name.split()) >= 2:  # Only add if at least first and last name\n",
    "                            people.add(name)\n",
    "        except Exception:\n",
    "            pass  # If NER fails, continue with regex patterns\n",
    "                            \n",
    "        # Method 2: Common name patterns (simplified)\n",
    "        name_patterns = [\n",
    "            # Title + Name: Mr. John Smith\n",
    "            r'(?:Mr|Mrs|Ms|Miss|Dr|Prof|Professor|Sir|Madam|Lady)\\.\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+',\n",
    "            # Name followed by credentials: John Smith, MD\n",
    "            r'[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+,\\s+(?:MD|PhD|JD|Esq)',\n",
    "            # Names in quotes: \"John Smith\"\n",
    "            r'\"[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+\"',\n",
    "            # Simple two-word capitalized names: John Smith\n",
    "            r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in name_patterns:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                name = match.group(0)\n",
    "                # Clean up the name (remove titles, credentials, quotes)\n",
    "                name = re.sub(r'^(?:Mr|Mrs|Ms|Miss|Dr|Prof|Professor|Sir|Madam|Lady)\\.\\s+', '', name)\n",
    "                name = re.sub(r',\\s+(?:MD|PhD|JD|Esq)$', '', name)\n",
    "                name = name.replace('\"', '')\n",
    "                people.add(name.strip())\n",
    "        \n",
    "        return list(people)\n",
    "    \n",
    "    def extract_sources(self, text: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extract source information such as URLs, email addresses,\n",
    "        references, and citations from text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        sources = []\n",
    "        \n",
    "        # URL pattern\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+(?:/[-\\w%!./?=&+]*)?'\n",
    "        urls = re.findall(url_pattern, text)\n",
    "        for url in urls:\n",
    "            sources.append({\n",
    "                'type': 'url',\n",
    "                'value': url\n",
    "            })\n",
    "        \n",
    "        # Email pattern\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, text)\n",
    "        for email in emails:\n",
    "            sources.append({\n",
    "                'type': 'email',\n",
    "                'value': email\n",
    "            })\n",
    "        \n",
    "        # Citation patterns\n",
    "        citation_patterns = [\n",
    "            # APA-like: (Author, 2023)\n",
    "            r'\\([A-Z][a-z]+(?:\\s+et\\s+al\\.?)?,\\s+\\d{4}\\)',\n",
    "            # MLA-like: (Author 2023)\n",
    "            r'\\([A-Z][a-z]+\\s+\\d{4}\\)',\n",
    "            # IEEE-like: [1], [2], etc.\n",
    "            r'\\[\\d+\\]',\n",
    "            # Footnote-like: [a], [b], etc. or superscript numbers\n",
    "            r'\\[[a-z]\\]|\\[\\d+\\]'\n",
    "        ]\n",
    "        \n",
    "        for pattern in citation_patterns:\n",
    "            citations = re.findall(pattern, text)\n",
    "            for citation in citations:\n",
    "                sources.append({\n",
    "                    'type': 'citation',\n",
    "                    'value': citation.strip()\n",
    "                })\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    def extract_comprehensive_metadata(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract comprehensive metadata from text including dates, \n",
    "        people, sources, and other useful information.\n",
    "        \"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Extract dates\n",
    "        dates = self.extract_dates(text)\n",
    "        if dates:\n",
    "            metadata['dates'] = dates\n",
    "            # Set the primary date (most recent)\n",
    "            try:\n",
    "                metadata['primary_date'] = sorted(dates)[-1]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Extract people\n",
    "        people = self.extract_people(text)\n",
    "        if people:\n",
    "            metadata['people'] = people\n",
    "            # The first person might be the author in many contexts\n",
    "            metadata['potential_author'] = people[0]\n",
    "        \n",
    "        # Extract sources\n",
    "        sources = self.extract_sources(text)\n",
    "        if sources:\n",
    "            metadata['sources'] = sources\n",
    "        \n",
    "        # Add text statistics\n",
    "        metadata['word_count'] = len(text.split())\n",
    "        metadata['char_count'] = len(text)\n",
    "        \n",
    "        # Add language detection (simple approach, just assuming English)\n",
    "        metadata['language'] = 'english'\n",
    "        \n",
    "        # Add extraction timestamp\n",
    "        metadata['metadata_extracted'] = datetime.datetime.now().isoformat()\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def categorize_conversations(self, texts: List[str], num_categories: int = 5) -> List[int]:\n",
    "        \"\"\"Categorize conversations using K-means clustering\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        kmeans = KMeans(n_clusters=num_categories, random_state=42)\n",
    "        return kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        return self.model.encode(texts)\n",
    "\n",
    "def exponential_backoff_retry(operation_func, max_retries=3, base_delay=1, max_delay=8):\n",
    "    \"\"\"\n",
    "    Execute the operation with exponential backoff retry logic.\n",
    "    \n",
    "    Args:\n",
    "        operation_func: Function to execute (should return a value or raise an exception)\n",
    "        max_retries: Maximum number of retries before giving up\n",
    "        base_delay: Initial delay in seconds\n",
    "        max_delay: Maximum delay in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Result of the operation function or None if all retries failed\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            if retries > 0:\n",
    "                logger.info(f\"Retry attempt {retries}/{max_retries}\")\n",
    "            \n",
    "            return operation_func()\n",
    "            \n",
    "        except Exception as e:\n",
    "            if retries == max_retries:\n",
    "                logger.error(f\"Operation failed after {max_retries} retries: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate delay with exponential backoff and jitter\n",
    "            delay = min(base_delay * (2 ** retries) + random.uniform(0, 1), max_delay)\n",
    "            \n",
    "            logger.warning(f\"Operation failed: {str(e)}. Retrying in {delay:.2f} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            retries += 1\n",
    "\n",
    "class PineconeManager:\n",
    "    def __init__(self, api_key: str, environment: str, index_name: str, dimension: int = 384):\n",
    "        self.api_key = api_key\n",
    "        self.environment = environment\n",
    "        self.index_name = index_name\n",
    "        self.dimension = dimension\n",
    "        logger.info(\"Initializing PineconeManager with index %s\", index_name)\n",
    "        self.initialize_pinecone()\n",
    "        \n",
    "    def initialize_pinecone(self):\n",
    "        \"\"\"Initialize Pinecone connection and index\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Connecting to Pinecone (environment: %s)\", self.environment)\n",
    "            pinecone.init(api_key=self.api_key, environment=self.environment)\n",
    "            \n",
    "            # Check if index exists, if not create it\n",
    "            if self.index_name not in pinecone.list_indexes():\n",
    "                logger.info(\"Creating new Pinecone index: %s\", self.index_name)\n",
    "                \n",
    "                def create_index_operation():\n",
    "                    pinecone.create_index(\n",
    "                        name=self.index_name,\n",
    "                        dimension=self.dimension,\n",
    "                        metric=\"cosine\"\n",
    "                    )\n",
    "                    return True\n",
    "                \n",
    "                result = exponential_backoff_retry(create_index_operation)\n",
    "                if result:\n",
    "                    logger.info(\"Successfully created index %s\", self.index_name)\n",
    "                else:\n",
    "                    raise Exception(\"Failed to create Pinecone index after retries\")\n",
    "            else:\n",
    "                logger.info(\"Using existing Pinecone index: %s\", self.index_name)\n",
    "            \n",
    "            self.index = pinecone.Index(self.index_name)\n",
    "            logger.info(\"Successfully connected to Pinecone index: %s\", self.index_name)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to initialize Pinecone: %s\", str(e), exc_info=True)\n",
    "            print(f\"ERROR initializing Pinecone: {str(e)}\")\n",
    "            print(\"Please check your Pinecone API key and environment.\")\n",
    "            raise\n",
    "    \n",
    "    def upsert_vectors(self, vectors: List[Dict[str, Any]]):\n",
    "        \"\"\"Insert or update vectors in the index\"\"\"\n",
    "        try:\n",
    "            if not vectors:\n",
    "                logger.warning(\"Empty vector list provided to upsert_vectors\")\n",
    "                print(\"Warning: Empty vector list provided to upsert_vectors\")\n",
    "                return True\n",
    "            \n",
    "            logger.info(\"Upserting %d vectors to Pinecone\", len(vectors))\n",
    "            \n",
    "            def upsert_operation():\n",
    "                self.index.upsert(vectors=vectors)\n",
    "                return True\n",
    "            \n",
    "            result = exponential_backoff_retry(upsert_operation)\n",
    "            \n",
    "            if result:\n",
    "                logger.info(\"Successfully upserted vectors to Pinecone\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"Failed to upsert vectors to Pinecone after retries\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to upsert vectors to Pinecone: %s\", str(e), exc_info=True)\n",
    "            print(f\"ERROR upserting vectors to Pinecone: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def query(self, query_vector: List[float], top_k: int = 5) -> Dict:\n",
    "        \"\"\"Query the index with a vector\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Querying Pinecone with top_k=%d\", top_k)\n",
    "            \n",
    "            def query_operation():\n",
    "                return self.index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
    "            \n",
    "            results = exponential_backoff_retry(query_operation)\n",
    "            \n",
    "            if results:\n",
    "                logger.info(\"Successfully queried Pinecone, found %d matches\", len(results.get(\"matches\", [])))\n",
    "                return results\n",
    "            else:\n",
    "                logger.error(\"Failed to query Pinecone after retries\")\n",
    "                return {\"matches\": []}\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to query Pinecone: %s\", str(e), exc_info=True)\n",
    "            print(f\"ERROR querying Pinecone: {str(e)}\")\n",
    "            # Return empty results structure on error\n",
    "            return {\"matches\": []}\n",
    "\n",
    "def count_conversations(json_file_path: str):\n",
    "    \"\"\"Count the number of conversations and messages in the JSON file\"\"\"\n",
    "    # Initialize processor\n",
    "    processor = DataProcessor()\n",
    "    \n",
    "    # Load conversations\n",
    "    conversations = processor.load_conversations(json_file_path)\n",
    "    \n",
    "    # Count conversations\n",
    "    num_conversations = len(conversations)\n",
    "    \n",
    "    # Count messages\n",
    "    total_messages = 0\n",
    "    for convo in conversations:\n",
    "        if \"messages\" in convo:\n",
    "            total_messages += len(convo[\"messages\"])\n",
    "    \n",
    "    return {\n",
    "        \"num_conversations\": num_conversations,\n",
    "        \"total_messages\": total_messages\n",
    "    }\n",
    "\n",
    "def process_and_store_conversations(json_file_path: str = None):\n",
    "    \"\"\"Process conversations and store in Pinecone\"\"\"\n",
    "    # Use provided path or get the default path\n",
    "    if json_file_path is None:\n",
    "        json_file_path = get_data_path()\n",
    "    \n",
    "    logger.info(\"Processing data from: %s\", json_file_path)\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = DataProcessor()\n",
    "    \n",
    "    # Load conversations\n",
    "    logger.info(\"Loading conversations from file\")\n",
    "    conversations = processor.load_conversations(json_file_path)\n",
    "    \n",
    "    # Count and print statistics\n",
    "    stats = count_conversations(json_file_path)\n",
    "    \n",
    "    logger.info(\"Processing %d conversations with %d total messages\", \n",
    "                stats['num_conversations'], stats['total_messages'])\n",
    "    print(f\"Processing {stats['num_conversations']} conversations with {stats['total_messages']} total messages\")\n",
    "    \n",
    "    # Extract texts for processing\n",
    "    logger.info(\"Extracting message texts\")\n",
    "    texts = []\n",
    "    for convo in conversations:\n",
    "        if \"messages\" in convo:\n",
    "            for message in convo[\"messages\"]:\n",
    "                if \"content\" in message:\n",
    "                    texts.append(message[\"content\"])\n",
    "    \n",
    "    # Preprocess texts\n",
    "    logger.info(\"Preprocessing %d texts\", len(texts))\n",
    "    print(\"Preprocessing texts...\")\n",
    "    preprocessed_texts = [processor.preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Extract keywords\n",
    "    logger.info(\"Extracting keywords\")\n",
    "    print(\"Extracting keywords...\")\n",
    "    keywords_dict = processor.extract_keywords(preprocessed_texts)\n",
    "    \n",
    "    # Categorize conversations\n",
    "    logger.info(\"Categorizing texts\")\n",
    "    print(\"Categorizing texts...\")\n",
    "    categories = processor.categorize_conversations(preprocessed_texts)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    logger.info(\"Generating embeddings for %d texts\", len(preprocessed_texts))\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = processor.generate_embeddings(preprocessed_texts)\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    logger.info(\"Initializing Pinecone connection\")\n",
    "    print(\"Initializing Pinecone...\")\n",
    "    pinecone_manager = PineconeManager(\n",
    "        api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "        environment=os.getenv(\"PINECONE_ENVIRONMENT\"),\n",
    "        index_name=\"gpt-conversations\"\n",
    "    )\n",
    "    \n",
    "    # Prepare vectors for Pinecone\n",
    "    logger.info(\"Preparing vectors with metadata\")\n",
    "    print(\"Preparing vectors...\")\n",
    "    vectors = []\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        # Extract comprehensive metadata\n",
    "        progress_pct = (i + 1) / len(embeddings) * 100\n",
    "        print(f\"Extracting metadata: {i+1}/{len(embeddings)} ({progress_pct:.1f}%)\", end=\"\\r\")\n",
    "        \n",
    "        if i % 10 == 0:  # Log every 10th item to avoid excessive logging\n",
    "            logger.debug(\"Processing document %d/%d\", i+1, len(embeddings))\n",
    "            \n",
    "        enriched_metadata = processor.extract_comprehensive_metadata(texts[i])\n",
    "        \n",
    "        # Combine with existing metadata\n",
    "        metadata = {\n",
    "            \"text\": texts[i],\n",
    "            \"keywords\": keywords_dict.get(i, []),\n",
    "            \"category\": int(categories[i])\n",
    "        }\n",
    "        \n",
    "        # Merge with enriched metadata\n",
    "        metadata.update(enriched_metadata)\n",
    "        \n",
    "        vector = {\n",
    "            \"id\": str(i),\n",
    "            \"values\": embedding.tolist(),\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    print(\"\\nUploading vectors to Pinecone...\")\n",
    "    logger.info(\"Uploading %d vectors to Pinecone in batches\", len(vectors))\n",
    "    \n",
    "    # Store vectors in batches (Pinecone has a limit)\n",
    "    batch_size = 100\n",
    "    total_batches = (len(vectors) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        end_idx = min(i + batch_size, len(vectors))\n",
    "        \n",
    "        # Calculate progress percentage\n",
    "        progress_pct = batch_num / total_batches * 100\n",
    "        \n",
    "        # Log the batch upload\n",
    "        logger.info(\"Uploading batch %d/%d (%d vectors)\", batch_num, total_batches, end_idx - i)\n",
    "        \n",
    "        # Display progress\n",
    "        print(f\"Uploading batch {batch_num}/{total_batches} ({progress_pct:.1f}%)...\", end=\"\\r\")\n",
    "        \n",
    "        # Upload the batch\n",
    "        success = pinecone_manager.upsert_vectors(vectors[i:end_idx])\n",
    "        \n",
    "        if not success:\n",
    "            logger.error(\"Failed to upload batch %d/%d\", batch_num, total_batches)\n",
    "            print(f\"\\nError uploading batch {batch_num}. Please check the logs for details.\")\n",
    "    \n",
    "    logger.info(\"Data processing and upload complete\")\n",
    "    print(\"\\nData processing complete!\")\n",
    "    return \"Data processed and stored successfully with enhanced metadata!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot System with Pinecone and Mobile Interface - Part 2: RAG Chatbot Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: RAG Chatbot Backend\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import openai\n",
    "import uvicorn\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "import os\n",
    "from llama_model import LlamaInstructModel\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"RAG Chatbot API\")\n",
    "\n",
    "# Add CORS middleware to allow cross-origin requests from the frontend\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Adjust in production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Add a middleware to log all requests\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    start_time = datetime.datetime.now()\n",
    "    path = request.url.path\n",
    "    method = request.method\n",
    "    \n",
    "    # Skip logging static file requests to reduce noise\n",
    "    if path.startswith(\"/static\"):\n",
    "        return await call_next(request)\n",
    "    \n",
    "    logger.info(f\"Request: {method} {path}\")\n",
    "    \n",
    "    try:\n",
    "        response = await call_next(request)\n",
    "        process_time = (datetime.datetime.now() - start_time).total_seconds() * 1000\n",
    "        logger.info(f\"Response: {method} {path} - Status: {response.status_code} - Time: {process_time:.2f}ms\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing request {method} {path}: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Set up templates for the web interface\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# Serve static files\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Initialize processor and Pinecone manager\n",
    "processor = DataProcessor()\n",
    "pinecone_manager = PineconeManager(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "    environment=os.getenv(\"PINECONE_ENVIRONMENT\"),\n",
    "    index_name=\"gpt-conversations\"\n",
    ")\n",
    "\n",
    "# Initialize the Llama model (lazily - will be created when first needed)\n",
    "llama_model = None\n",
    "\n",
    "def get_llama_model():\n",
    "    global llama_model\n",
    "    if llama_model is None:\n",
    "        logger.info(\"Initializing Llama model for the first time\")\n",
    "        # Choose the appropriate model - you can change to a different Llama variant\n",
    "        model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # A free model on Hugging Face\n",
    "        llama_model = LlamaInstructModel(model_name=model_name)\n",
    "    return llama_model\n",
    "\n",
    "# Define request and response models\n",
    "class ChatRequest(BaseModel):\n",
    "    query: str\n",
    "    history: Optional[List[dict]] = []\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    sources: List[dict]\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def get_web_interface(request: Request):\n",
    "    \"\"\"Serve the web interface\"\"\"\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Process a chat request using RAG\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Received chat request: '%s'\", request.query[:50] + \"...\" if len(request.query) > 50 else request.query)\n",
    "        \n",
    "        # Preprocess query\n",
    "        processed_query = processor.preprocess_text(request.query)\n",
    "        logger.debug(\"Preprocessed query: '%s'\", processed_query)\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        logger.info(\"Generating embedding for query\")\n",
    "        query_embedding = processor.model.encode(processed_query).tolist()\n",
    "        \n",
    "        # Retrieve relevant documents from Pinecone\n",
    "        logger.info(\"Retrieving relevant documents from Pinecone\")\n",
    "        results = pinecone_manager.query(query_vector=query_embedding, top_k=3)\n",
    "        \n",
    "        # Extract contexts from the results\n",
    "        contexts = []\n",
    "        for match in results.get(\"matches\", []):\n",
    "            if \"metadata\" in match and \"text\" in match[\"metadata\"]:\n",
    "                contexts.append(match[\"metadata\"][\"text\"])\n",
    "        \n",
    "        logger.info(\"Retrieved %d relevant contexts\", len(contexts))\n",
    "        \n",
    "        # Construct prompt with retrieved contexts\n",
    "        system_prompt = \"You are a helpful assistant. Answer the question accurately based on the provided context.\"\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "        Answer based on the following contexts:\n",
    "        \n",
    "        {' '.join(contexts)}\n",
    "        \n",
    "        Chat History:\n",
    "        {' '.join([f\"{msg['role']}: {msg['content']}\" for msg in request.history])}\n",
    "        \n",
    "        Question: {request.query}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate response using Llama model\n",
    "        logger.info(\"Generating response using Llama model\")\n",
    "        llama_model = get_llama_model()\n",
    "        answer = llama_model.generate_response(system_prompt, user_prompt)\n",
    "        \n",
    "        logger.info(\"Generated response: '%s'\", answer[:50] + \"...\" if len(answer) > 50 else answer)\n",
    "        \n",
    "        # Format sources for the response\n",
    "        sources = []\n",
    "        for i, match in enumerate(results.get(\"matches\", [])[:3]):\n",
    "            if \"metadata\" in match:\n",
    "                sources.append({\n",
    "                    \"text\": match[\"metadata\"].get(\"text\", \"\"),\n",
    "                    \"score\": float(match.get(\"score\", 0)),\n",
    "                    \"keywords\": match[\"metadata\"].get(\"keywords\", []),\n",
    "                    \"category\": match[\"metadata\"].get(\"category\", -1)\n",
    "                })\n",
    "        \n",
    "        logger.info(\"Returning response with %d sources\", len(sources))\n",
    "        return ChatResponse(response=answer, sources=sources)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in chat endpoint: %s\", str(e), exc_info=True)\n",
    "        print(f\"Error in chat endpoint: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Function to start the server\n",
    "def start_server():\n",
    "    \"\"\"Start the FastAPI server\"\"\"\n",
    "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot System with Pinecone and Mobile Interface - Part 3: Frontend Web Interface (HTML, CSS, JS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Frontend Web Interface (HTML, CSS, JS)\n",
    "\n",
    "# Create templates directory if it doesn't exist\n",
    "os.makedirs(\"templates\", exist_ok=True)\n",
    "os.makedirs(\"static\", exist_ok=True)\n",
    "\n",
    "# Create index.html\n",
    "index_html = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>RAG Chatbot</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/styles.css\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"chat-header\">\n",
    "            <h1>RAG Chatbot</h1>\n",
    "        </div>\n",
    "        <div class=\"chat-messages\" id=\"chat-messages\">\n",
    "            <div class=\"message bot\">\n",
    "                <div class=\"message-content\">Hello! How can I help you today?</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div class=\"chat-input\">\n",
    "            <input type=\"text\" id=\"user-input\" placeholder=\"Type your message here...\">\n",
    "            <button id=\"send-button\">Send</button>\n",
    "        </div>\n",
    "        <div class=\"sources-container\" id=\"sources-container\">\n",
    "            <h3>Sources</h3>\n",
    "            <div class=\"sources-list\" id=\"sources-list\"></div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <script src=\"/static/script.js\"></script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "# Create styles.css\n",
    "styles_css = \"\"\"* {\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    box-sizing: border-box;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "}\n",
    "\n",
    "body {\n",
    "    background-color: #f5f5f5;\n",
    "    height: 100vh;\n",
    "    display: flex;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "}\n",
    "\n",
    "chat-container {\n",
    "    width: 100%;\n",
    "    max-width: 600px;\n",
    "    height: 100vh;\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    background-color: white;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);\n",
    "    overflow: hidden;\n",
    "}\n",
    "\n",
    "chat-header {\n",
    "    padding: 15px;\n",
    "    background-color: #4a6fa5;\n",
    "    color: white;\n",
    "    text-align: center;\n",
    "}\n",
    "\n",
    "chat-messages {\n",
    "    flex: 1;\n",
    "    padding: 15px;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "\n",
    "message {\n",
    "    margin-bottom: 15px;\n",
    "    display: flex;\n",
    "}\n",
    "\n",
    "user {\n",
    "    justify-content: flex-end;\n",
    "}\n",
    "\n",
    "bot {\n",
    "    justify-content: flex-start;\n",
    "}\n",
    "\n",
    "message-content {\n",
    "    padding: 10px 15px;\n",
    "    border-radius: 20px;\n",
    "    max-width: 70%;\n",
    "    word-break: break-word;\n
